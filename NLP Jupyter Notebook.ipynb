{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''!pip install wordcloud\n",
    "!pip install fileupload\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension install --py --user fileupload\n",
    "!jupyter nbextension enable --py fileupload\n",
    "'''\n",
    "import wordcloud\n",
    "import re\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "import fileupload\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "##open the text document T1\n",
    "f1 = open('Documents/Crime&Punishment.txt','r',encoding='utf-8')\n",
    "##reading the text document T1\n",
    "T1=f1.read()\n",
    "##tokenizing(converting the words and characters in the form of list) the text document to remove /n /r and chapter number and names\n",
    "T1_tokens = word_tokenize(T1) \n",
    "##Preprocessing step for converting all the tokens in lowercase\n",
    "T1_tokens = [i.lower() for i in T1_tokens]\n",
    "##Preprocessing step - Lemmitization \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "T1_tokens=[lemmatizer.lemmatize(i) for i in T1_tokens]\n",
    "\n",
    "##rcode snipet for removal of punctuation marks\n",
    "T1_useful = []\n",
    "for i in T1_tokens:\n",
    "    if re.search(\"[a-zA-Z]+\", i) != None:\n",
    "        T1_useful.append(i)\n",
    "        \n",
    "##Plotting top 40 most frequent words \n",
    "def plot_freq(result):\n",
    "    plt.bar(range(len(result.keys())), list(result.values()), align='center',width=0.3)\n",
    "    plt.xticks(range(len(result)), list(result.keys()),rotation='vertical')\n",
    "    plt.figure(figsize=(100,100))\n",
    "    plt.show()\n",
    "    \n",
    "##Generates frequenry distribution of every word in the list of tokens/corpus and generates its corresponding word cloud\n",
    "\n",
    "def calculate_frequencies(tokens):\n",
    "    # Here is a list of punctuations and uninteresting words you can use to process your text\n",
    "    #punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    \n",
    "    \n",
    "    T1_freq = {}\n",
    "    for word in T1_tokens:\n",
    "        if word not in T1_freq.keys():\n",
    "            T1_freq[word]=1\n",
    "        else:\n",
    "            T1_freq[word]+=1\n",
    "    \n",
    "    \n",
    "    sorted_freq={k: v for k, v in sorted(T1_freq.items(), key=lambda item: item[1],reverse=True)}\n",
    "    sorted_freq=dict(itertools.islice(sorted_freq.items(), 40))\n",
    "    plot_freq(sorted_freq)\n",
    "    \n",
    "\n",
    "    \n",
    "    #wordcloud\n",
    "    cloud = wordcloud.WordCloud()\n",
    "    cloud.generate_from_frequencies(T1_freq)\n",
    "    return cloud.to_array()\n",
    "\n",
    "#plotting the word cloud with stop words\n",
    "myimage1 = calculate_frequencies(T1_useful)\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(myimage1, interpolation = 'nearest')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#Generates frequenry distribution of every word (excluding stop words) in the list of tokens/corpus and generates its corresponding word cloud \n",
    "\n",
    "def calculate_freq_without_stop_words(tokens):\n",
    "    \n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    \n",
    "    T1_wfreq = {}\n",
    "    for words in T1_tokens:\n",
    "        if words in stop_words or words in punctuations:\n",
    "            pass\n",
    "        else:\n",
    "            if words not in T1_wfreq.keys():\n",
    "                T1_wfreq[words]=0\n",
    "            else:\n",
    "                T1_wfreq[words]+=1\n",
    "   \n",
    "    sorted_wfreq={k: v for k, v in sorted(T1_wfreq.items(), key=lambda item: item[1],reverse=True)}\n",
    "    sorted_wfreq=dict(itertools.islice(sorted_wfreq.items(), 40))\n",
    "    plot_freq(sorted_wfreq)\n",
    "    \n",
    "\n",
    "    \n",
    "    #wordcloud\n",
    "    clouds = wordcloud.WordCloud(background_color='black')\n",
    "    clouds.generate_from_frequencies(res)\n",
    "    return clouds.to_array()\n",
    "\n",
    "#plotting the word cloud without stop words\n",
    "myimage2 = calculate_freq_without_stop_words(T1_useful)\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(myimage2, interpolation = 'nearest')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#frequency distribution of words with respect to their word length\n",
    "T1_counts={}\n",
    "for wor in T1_useful:\n",
    "    if len(wor) not in T1_counts:\n",
    "        T1_counts[len(wor)] = 1\n",
    "    else:\n",
    "        T1_counts[len(wor)]+=1\n",
    "\n",
    "T1_counts = dict(sorted(T1_counts.items(), key=lambda x:x[0])) #Converts to a list of tuples and sorts\n",
    "\n",
    "#Used Genrators for the implementation of \"lazy\" to avoid memory overload\n",
    "def TaggerGen():\n",
    "    for i in T1_useful:\n",
    "        T1_postag = nltk.pos_tag(i.split())\n",
    "        yield T1_postag\n",
    "        \n",
    "tg = TaggerGen()\n",
    "\n",
    "filer = open(\"war&peace_tagged.txt\", \"a\")\n",
    "for i in range(len(T1_useful)):\n",
    "    out1,out2 = next(tg)[0]\n",
    "    r = out1+\" \"+out2+\"\\n\"\n",
    "    filer.write(r)\n",
    "filer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##open the text document T2\n",
    "f2 = open('Documents/war&peace.txt','r',encoding='utf-8')\n",
    "##reading the text document T2\n",
    "T2=f2.read()\n",
    "##tokenizing(converting the words and characters in the form of list) the text document to remove /n /r and chapter number and names\n",
    "T2_tokens = word_tokenize(T2) \n",
    "##Preprocessing step for converting all the tokens in lowercase\n",
    "T2_tokens = [i.lower() for i in T2_tokens]\n",
    "##Preprocessing step - Lemmitization \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "T2_tokens=[lemmatizer.lemmatize(i) for i in T2_tokens]\n",
    "\n",
    "##rcode snipet for removal of punctuation marks\n",
    "T2_useful = []\n",
    "for i in T2_tokens:\n",
    "    if re.search(\"[a-zA-Z]+\", i) != None:\n",
    "        T2_useful.append(i)\n",
    "        \n",
    "##Plotting top 40 most frequent words \n",
    "def plot_freq(result):\n",
    "    plt.bar(range(len(result.keys())), list(result.values()), align='center',width=0.3)\n",
    "    plt.xticks(range(len(result)), list(result.keys()),rotation='vertical')\n",
    "    plt.figure(figsize=(100,100))\n",
    "    plt.show()\n",
    "    \n",
    "##Generates frequenry distribution of every word in the list of tokens/corpus and generates its corresponding word cloud\n",
    "\n",
    "def calculate_frequencies(tokens):\n",
    "    # Here is a list of punctuations and uninteresting words you can use to process your text\n",
    "    #punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    \n",
    "    \n",
    "    T2_freq = {}\n",
    "    for word in T2_tokens:\n",
    "        if word not in T2_freq.keys():\n",
    "            T2_freq[word]=1\n",
    "        else:\n",
    "            T2_freq[word]+=1\n",
    "    \n",
    "    \n",
    "    sorted_freq2={k: v for k, v in sorted(T2_freq.items(), key=lambda item: item[1],reverse=True)}\n",
    "    sorted_freq2=dict(itertools.islice(sorted_freq2.items(), 40))\n",
    "    plot_freq(sorted_freq2)\n",
    "    \n",
    "\n",
    "    \n",
    "    #wordcloud\n",
    "    cloud = wordcloud.WordCloud()\n",
    "    cloud.generate_from_frequencies(T2_freq)\n",
    "    return cloud.to_array()\n",
    "    \n",
    "#plotting the word cloud with stop words\n",
    "myimage3 = calculate_frequencies(T2_useful)\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(myimage3, interpolation = 'nearest')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#Generates frequenry distribution of every word (excluding stop words) in the list of tokens/corpus and generates its corresponding word cloud \n",
    "\n",
    "def calculate_freq_without_stop_words(tokens):\n",
    "    \n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    \n",
    "    T2_wfreq = {}\n",
    "    for words in T2_tokens:\n",
    "        if words in stop_words or words in punctuations:\n",
    "            pass\n",
    "        else:\n",
    "            if words not in T2_wfreq.keys():\n",
    "                T2_wfreq[words]=0\n",
    "            else:\n",
    "                T2_wfreq[words]+=1\n",
    "   \n",
    "    sorted_wfreq2={k: v for k, v in sorted(T2_wfreq.items(), key=lambda item: item[1],reverse=True)}\n",
    "    sorted_wfreq2=dict(itertools.islice(sorted_wfreq2.items(), 40))\n",
    "    plot_freq(sorted_wfreq2)\n",
    "    \n",
    "\n",
    "    \n",
    "    #wordcloud\n",
    "    clouds = wordcloud.WordCloud(background_color='black')\n",
    "    clouds.generate_from_frequencies(T2_wref)\n",
    "    return clouds.to_array()\n",
    "    \n",
    "#plotting the word cloud without stop words\n",
    "myimage4 = calculate_freq_without_stop_words(T2_useful)\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(myimage4, interpolation = 'nearest')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#frequency distribution of words with respect to their word length\n",
    "T2_counts={}\n",
    "for wor in T2_useful:\n",
    "    if len(wor) not in T2_counts:\n",
    "        T2_counts[len(wor)] = 1\n",
    "    else:\n",
    "        T2_counts[len(wor)]+=1\n",
    "\n",
    "T2_counts = dict(sorted(T2_counts.items(), key=lambda x:x[0])) #Converts to a list of tuples and sorts\n",
    "\n",
    "#Used Genrators for the implementation of \"lazy\" to avoid memory overload\n",
    "def TaggerGen():\n",
    "    for i in T2_useful:\n",
    "        T2_postag = nltk.pos_tag(i.split())\n",
    "        yield T2_postag\n",
    "tg = TaggerGen()\n",
    "filer = open(\"war&peace_tagged.txt\", \"a\")\n",
    "for i in range(len(T2_useful)):\n",
    "    out1,out2 = next(tg)[0]\n",
    "    r = out1+\" \"+out2+\"\\n\"\n",
    "    filer.write(r)\n",
    "filer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
